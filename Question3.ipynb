{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|           Job Title|Avg Salary per job|\n",
      "+--------------------+------------------+\n",
      "|Senior Data Scien...| 99.33333333333333|\n",
      "|Clinical Data Ana...|             164.5|\n",
      "|Senior Business I...|              90.0|\n",
      "|Data Analyst/Engi...|             115.5|\n",
      "|Staff BI and Data...|             107.0|\n",
      "|Intelligence Data...|             90.75|\n",
      "|Report Writer-Dat...|              92.5|\n",
      "|Hydrogen/Tritium ...|             148.0|\n",
      "|Business Intellig...|            109.25|\n",
      "|        Data Modeler|             154.0|\n",
      "|Scientist / Group...|             197.5|\n",
      "|Senior Research S...|             105.0|\n",
      "|Software Engineer...|             164.5|\n",
      "|   Sr Data Scientist|            126.75|\n",
      "|COMPUTER SCIENTIS...|             271.5|\n",
      "|Data Scientist/Ma...|             125.5|\n",
      "|Data Scientist - ...|             120.5|\n",
      "|  Decision Scientist|              94.5|\n",
      "|Data Scientist - ...|             97.75|\n",
      "|Data Scientist / ...|             128.5|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+----------------------+\n",
      "|SizeCategory|avg_salary_on_category|\n",
      "+------------+----------------------+\n",
      "|      Medium|    120.93506493506493|\n",
      "|       Small|    122.80325814536342|\n",
      "|       Large|    124.48571428571428|\n",
      "+------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,isnan,when,count,mean,sum,split,isnull,avg\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Q3\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"Cleaned_DS_Jobs.csv\",header=True,inferSchema=True)\n",
    "\n",
    "\n",
    "df = df.withColumn(\"min_salary\", split(col(\"Salary Estimate\"), \"-\").getItem(0).cast(\"int\"))\n",
    "df = df.withColumn(\"max_salary\", split(col(\"Salary Estimate\"), \"-\").getItem(1).cast(\"int\"))\n",
    "\n",
    "df = df.withColumn(\"Avg Salary\",((col(\"min_salary\")+col(\"max_salary\"))/2).cast(\"double\"))\n",
    "\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Rating\",\n",
    "    when(col(\"Rating\") == -1, 1)\n",
    "    .when(col(\"Rating\") == 0, 1)\n",
    "    .otherwise(col(\"Rating\")),\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "null_check = df.select(\n",
    "    [\n",
    "        count(when(col(c).isNull() | isnan(c) | (col(c) == \"\"), c)).alias(c)\n",
    "        for c in df.columns\n",
    "    ]   \n",
    ")\n",
    "\n",
    "null_check.show()\n",
    "\"\"\"\n",
    "\n",
    "for column in df.columns:\n",
    "    df = df.withColumn(column, when(isnull(col(column)), -1).otherwise(col(column)))\n",
    "\n",
    "\n",
    "# Q4\n",
    "\n",
    "average_salary_per_job = df.groupBy(\"Job Title\").agg(avg(\"Avg Salary\").alias(\"Avg Salary per job\"))\n",
    "\n",
    "average_salary_per_job.show()\n",
    "\n",
    "\n",
    "# Q5\n",
    "\n",
    "df = df.filter((col(\"Size\")!=\"-1\") & (col(\"Size\")!=\"Unknown\"))\n",
    "\"\"\"\n",
    "\n",
    "distinct_elements = df.select(\"Size\").distinct().collect()\n",
    "\n",
    "\n",
    "for _ in distinct_elements:\n",
    "    print(_['Size'])\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "size_category = when(\n",
    "    col(\"Size\").rlike('1 to 50|51 to 200'), 'Small'\n",
    ").when(\n",
    "    col(\"Size\").rlike('201 to 500|501 to 1000|1001 to 5000'), 'Medium'\n",
    ").when(\n",
    "    col(\"Size\").rlike('5001 to 10000|10000\\+'), 'Large'\n",
    ").otherwise('Unknown')\n",
    "\n",
    "\n",
    "df = df.withColumn(\"SizeCategory\", size_category)\n",
    "\n",
    "avg_salary_on_category = df.groupBy(\"SizeCategory\").agg(avg(\"avg salary\").alias('avg_salary_on_category'))\n",
    "\n",
    "avg_salary_on_category.show()\n",
    "\n",
    "# average_salary_per_size = df.groupBy(\"Size\").agg(avg(\"Avg Salary\").alias(\"Avg Salary per Size\"))\n",
    "\n",
    "\n",
    "# average_salary_per_job.show()\n",
    "\n",
    "df.coalesce(1).write.csv(\"output3\",header=True,mode=\"overwrite\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
