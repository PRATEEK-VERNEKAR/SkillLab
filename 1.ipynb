{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/29 10:47:52 WARN Utils: Your hostname, prateek-HP-Pavilion-Laptop-15-eh1xxx resolves to a loopback address: 127.0.1.1; using 10.20.17.8 instead (on interface wlo1)\n",
      "24/12/29 10:47:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/29 10:47:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|product_name|total_sales|\n",
      "+------------+-----------+\n",
      "|   Product D|        470|\n",
      "|   Product A|        450|\n",
      "|   Product E|        400|\n",
      "|   Product B|        380|\n",
      "|   Product C|        290|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Top-N Products\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"1.csv\"  # Path to your CSV file\n",
    "sales_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Aggregating total sales by product_name\n",
    "aggregated_df = sales_df.groupBy(\"product_name\").sum(\"sales\").withColumnRenamed(\"sum(sales)\", \"total_sales\")\n",
    "\n",
    "# Ordering by total_sales in descending order and selecting top 5\n",
    "top_5_products = aggregated_df.orderBy(\"total_sales\", ascending=False).limit(5)\n",
    "\n",
    "# Show results\n",
    "top_5_products.show()\n",
    "\n",
    "# Stopping Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|product_name|total_sales|\n",
      "+------------+-----------+\n",
      "|   Product D|        470|\n",
      "|   Product A|        450|\n",
      "|   Product E|        400|\n",
      "|   Product B|        380|\n",
      "|   Product C|        290|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Top-N Produce\").getOrCreate()\n",
    "\n",
    "sales_df=spark.read.csv(\"1.csv\",header=True,inferSchema=True)\n",
    "\n",
    "aggregated_df=sales_df.groupBy(\"product_name\").sum(\"sales\").withColumnRenamed(\"sum(sales)\",\"total_sales\")\n",
    "\n",
    "top_5 = aggregated_df.orderBy(\"total_sales\",ascending=False).limit(5)\n",
    "\n",
    "top_5.show()\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+--------+--------+----------+\n",
      "|employee_id|quarter1|quarter2|quarter3|quarter4|max_quater|\n",
      "+-----------+--------+--------+--------+--------+----------+\n",
      "|          1|      85|      90|      88|      92|        85|\n",
      "|          2|      78|      81|      79|      85|        78|\n",
      "|          3|      92|      88|      91|      54|        54|\n",
      "|          4|      75|      80|      78|      82|        75|\n",
      "|          5|      89|      85|      87|      88|        85|\n",
      "+-----------+--------+--------+--------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import least\n",
    "\n",
    "spark = SparkSession.builder.appName(\"2nd Program\").getOrCreate()\n",
    "\n",
    "employee_df = spark.read.csv(\"2.csv\",header=True,inferSchema=True)\n",
    "\n",
    "result_df = employee_df.withColumn(\"max_quater\",least(\"quarter1\",\"quarter2\",\"quarter3\",\"quarter4\"))\n",
    "\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+\n",
      "|customer_id|spending|currency|\n",
      "+-----------+--------+--------+\n",
      "|          1|   150.5|     USD|\n",
      "|          2|   200.0|     USD|\n",
      "|          3|  120.75|     USD|\n",
      "|          4|   300.6|     USD|\n",
      "|          5|  250.25|     USD|\n",
      "+-----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "spark = SparkSession.builder.appName(\"3rd Program\").getOrCreate()\n",
    "\n",
    "customer_df = spark.read.csv(\"3.csv\",header=True,inferSchema=True)\n",
    "\n",
    "\n",
    "result_df = customer_df.withColumn(\"currency\",lit(\"USD\"))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+\n",
      "|   category|subcategory|total_sales|\n",
      "+-----------+-----------+-----------+\n",
      "|  Furniture|      Chair|        500|\n",
      "|  Furniture|      Table|        800|\n",
      "|Electronics|     Laptop|       1200|\n",
      "|Electronics|     Mobile|       1200|\n",
      "+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"4th Program\").getOrCreate()\n",
    "\n",
    "price_df = spark.read.csv(\"4.csv\",header=True,inferSchema=True)\n",
    "\n",
    "price_df.createOrReplaceTempView(\"price_view\")\n",
    "\n",
    "sqlquery = \"\"\"\n",
    "    SELECT category,subcategory,SUM(sales) AS total_sales\n",
    "    FROM price_view\n",
    "    GROUP BY category,subcategory\n",
    "    ORDER BY total_sales ASC\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "result_df = spark.sql(sqlquery)\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
